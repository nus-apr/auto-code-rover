{
    "task_id": "scikit-learn__scikit-learn-13142",
    "setup_info": {
        "repo_path": "/home/haifeng/projects/reverse-prompt/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21",
        "env_name": "setup_scikit-learn__scikit-learn__0.21",
        "pre_install": [],
        "install": "python -m pip install -v --no-use-pep517 --no-build-isolation -e .",
        "test_cmd": "pytest --no-header -rA --tb=no -p no:cacheprovider sklearn/mixture/tests/test_bayesian_mixture.py sklearn/mixture/tests/test_gaussian_mixture.py"
    },
    "task_info": {
        "instance_id": "scikit-learn__scikit-learn-13142",
        "base_commit": "1c8668b0a021832386470ddf740d834e02c66f69",
        "hints_text": "Indeed the code in fit_predict and the one in predict are not exactly consistent. This should be fixed but we would need to check the math to choose the correct variant, add a test and remove the other one.\nI don't think the math is wrong or inconsistent.  I think it's a matter of `fit_predict` returning the fit from the last of `n_iter` iterations, when it should be returning the fit from the _best_ of the iterations.  That is, the last call to `self._e_step()` (base.py:263) should be moved to just before the return, after `self._set_parameters(best_params)` restores the best solution.\nSeems good indeed. When looking quickly you can miss the fact that `_e_step` uses the parameters even if not passed as arguments because they are attributes of the estimator. That's what happened to me :)\r\n\r\n Would you submit a PR ?",
        "created_at": "2019-02-12T14:32:37Z",
        "test_patch": "diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py\n--- a/sklearn/mixture/tests/test_bayesian_mixture.py\n+++ b/sklearn/mixture/tests/test_bayesian_mixture.py\n@@ -451,6 +451,15 @@ def test_bayesian_mixture_fit_predict(seed, max_iter, tol):\n         assert_array_equal(Y_pred1, Y_pred2)\n \n \n+def test_bayesian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_bayesian_mixture_predict_predict_proba():\n     # this is the same test as test_gaussian_mixture_predict_predict_proba()\n     rng = np.random.RandomState(0)\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -598,6 +598,15 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n \n \n+def test_gaussian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = GaussianMixture(n_components=5, n_init=5, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_gaussian_mixture_fit():\n     # recover the ground truth\n     rng = np.random.RandomState(0)\n",
        "repo": "scikit-learn/scikit-learn",
        "problem_statement": "GaussianMixture predict and fit_predict disagree when n_init>1\n#### Description\r\nWhen `n_init` is specified in GaussianMixture, the results of fit_predict(X) and predict(X) are often different.  The `test_gaussian_mixture_fit_predict` unit test doesn't catch this because it does not set `n_init`.\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\npython\r\nfrom sklearn.mixture import GaussianMixture\r\nfrom sklearn.utils.testing import assert_array_equal\r\nimport numpy\r\nX = numpy.random.randn(1000,5)\r\nprint 'no n_init'\r\ngm = GaussianMixture(n_components=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\nprint 'n_init=5'\r\ngm = GaussianMixture(n_components=5, n_init=5)\r\nc1 = gm.fit_predict(X)\r\nc2 = gm.predict(X)\r\nassert_array_equal(c1,c2)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nno n_init\r\nn_init=5\r\n```\r\nNo exceptions.\r\n\r\n#### Actual Results\r\n```\r\nno n_init\r\nn_init=5\r\nTraceback (most recent call last):\r\n  File \"test_gm.py\", line 17, in <module>\r\n    assert_array_equal(c1,c2)\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 872, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/home/scott/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 88.6%)\r\n x: array([4, 0, 1, 1, 1, 3, 3, 4, 4, 2, 0, 0, 1, 2, 0, 2, 0, 1, 3, 1, 1, 3,\r\n       2, 1, 0, 2, 1, 0, 2, 0, 3, 1, 2, 3, 3, 1, 0, 2, 2, 0, 3, 0, 2, 0,\r\n       4, 2, 3, 0, 4, 2, 4, 1, 0, 2, 2, 1, 3, 2, 1, 4, 0, 2, 2, 1, 1, 2,...\r\n y: array([4, 1, 0, 2, 2, 1, 1, 4, 4, 0, 4, 1, 0, 3, 1, 0, 2, 2, 1, 2, 0, 0,\r\n       1, 0, 4, 1, 0, 4, 0, 1, 1, 2, 3, 1, 4, 0, 1, 4, 4, 4, 0, 1, 0, 2,\r\n       4, 1, 1, 2, 4, 3, 4, 0, 2, 3, 2, 3, 0, 0, 2, 3, 3, 3, 3, 0, 3, 2,...\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 2.7.15rc1 (default, Nov 12 2018, 14:31:15)  [GCC 7.3.0]\r\n   machine: Linux-4.15.0-43-generic-x86_64-with-Ubuntu-18.04-bionic\r\nexecutable: /usr/bin/python\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\ncblas_libs: cblas\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\n\r\nPython deps:\r\n    Cython: 0.28.5\r\n     scipy: 1.2.0\r\nsetuptools: 39.0.1\r\n       pip: 19.0.1\r\n     numpy: 1.16.0\r\n    pandas: 0.23.1\r\n   sklearn: 0.20.2\r\n```\n",
        "version": "0.21",
        "FAIL_TO_PASS": [
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict_n_init",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict_n_init"
        ],
        "PASS_TO_PASS": [
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_check_is_fitted",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_covariance_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[0-2-1e-07]",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[1-2-0.1]",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[3-300-1e-07]",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict[4-300-0.1]",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_mean_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_precisions_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_predict_predict_proba",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weight_concentration_prior_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_check_covariance_precision",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_compare_covar_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_invariant_translation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_log_dirichlet_norm",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_log_wishart_norm",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_monotonic_likelihood",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_bic_1d_1component",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_X",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_means",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_precisions",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_weights",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_compute_log_det_cholesky",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_convergence_detected_with_warm_start",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_aic_bic",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_attributes",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_estimate_log_prob_resp",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_best_params",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_convergence_warning",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[0-2-1e-07]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[1-2-0.1]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[3-300-1e-07]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict[4-300-0.1]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_log_probabilities",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_n_parameters",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_predict_predict_proba",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_verbose",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_suffstat_sk_spherical",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_init",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_monotonic_likelihood",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_multiple_init",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_property",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_regularisation",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_sample",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_score",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_score_samples",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_diag",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_full",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_tied",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[0]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[1]",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start[2]"
        ],
        "environment_setup_commit": "7813f7efb5b2012412888b69e73d76f2df2b50b6",
        "patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -257,11 +257,6 @@ def fit_predict(self, X, y=None):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,6 +268,11 @@ def fit_predict(self, X, y=None):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        # Always do a final e-step to guarantee that the labels returned by\n+        # fit_predict(X) are always consistent with fit(X).predict(X)\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n",
        "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13142"
    }
}