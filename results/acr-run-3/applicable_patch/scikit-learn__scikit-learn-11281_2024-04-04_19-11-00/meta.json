{
    "task_id": "scikit-learn__scikit-learn-11281",
    "setup_info": {
        "repo_path": "/media/media0/yuntong/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.20",
        "env_name": "setup_scikit-learn__scikit-learn__0.20",
        "pre_install": [],
        "install": "python -m pip install -v --no-use-pep517 --no-build-isolation -e .",
        "test_cmd": "pytest --no-header -rA --tb=no -p no:cacheprovider sklearn/mixture/tests/test_bayesian_mixture.py sklearn/mixture/tests/test_gaussian_mixture.py"
    },
    "task_info": {
        "instance_id": "scikit-learn__scikit-learn-11281",
        "base_commit": "4143356c3c51831300789e4fdf795d83716dbab6",
        "hints_text": "In my opinion, yes.\r\n\r\nI wanted to compare K-Means, GMM and HDBSCAN and was very disappointed that GMM does not have a `fit_predict` method. The HDBSCAN examples use `fit_predict`, so I was expecting GMM to have the same interface.\nI think we should add ``fit_predict`` at least. I wouldn't rename ``n_components``.\nI would like to work on this!\n@Eight1911 go for it. It is probably relatively simple but maybe not entirely trivial.\n@Eight1911 Mind if I take a look at this?\n@Eight1911 Do you mind if I jump in as well?",
        "created_at": "2018-06-15T17:15:25Z",
        "test_patch": "diff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py\n--- a/sklearn/mixture/tests/test_bayesian_mixture.py\n+++ b/sklearn/mixture/tests/test_bayesian_mixture.py\n@@ -1,12 +1,16 @@\n # Author: Wei Xue <xuewei4d@gmail.com>\n #         Thierry Guillemot <thierry.guillemot.work@gmail.com>\n # License: BSD 3 clause\n+import copy\n \n import numpy as np\n from scipy.special import gammaln\n \n from sklearn.utils.testing import assert_raise_message\n from sklearn.utils.testing import assert_almost_equal\n+from sklearn.utils.testing import assert_array_equal\n+\n+from sklearn.metrics.cluster import adjusted_rand_score\n \n from sklearn.mixture.bayesian_mixture import _log_dirichlet_norm\n from sklearn.mixture.bayesian_mixture import _log_wishart_norm\n@@ -14,7 +18,7 @@\n from sklearn.mixture import BayesianGaussianMixture\n \n from sklearn.mixture.tests.test_gaussian_mixture import RandomData\n-from sklearn.exceptions import ConvergenceWarning\n+from sklearn.exceptions import ConvergenceWarning, NotFittedError\n from sklearn.utils.testing import assert_greater_equal, ignore_warnings\n \n \n@@ -419,3 +423,49 @@ def test_invariant_translation():\n             assert_almost_equal(bgmm1.means_, bgmm2.means_ - 100)\n             assert_almost_equal(bgmm1.weights_, bgmm2.weights_)\n             assert_almost_equal(bgmm1.covariances_, bgmm2.covariances_)\n+\n+\n+def test_bayesian_mixture_fit_predict():\n+    rng = np.random.RandomState(0)\n+    rand_data = RandomData(rng, scale=7)\n+    n_components = 2 * rand_data.n_components\n+\n+    for covar_type in COVARIANCE_TYPE:\n+        bgmm1 = BayesianGaussianMixture(n_components=n_components,\n+                                        max_iter=100, random_state=rng,\n+                                        tol=1e-3, reg_covar=0)\n+        bgmm1.covariance_type = covar_type\n+        bgmm2 = copy.deepcopy(bgmm1)\n+        X = rand_data.X[covar_type]\n+\n+        Y_pred1 = bgmm1.fit(X).predict(X)\n+        Y_pred2 = bgmm2.fit_predict(X)\n+        assert_array_equal(Y_pred1, Y_pred2)\n+\n+\n+def test_bayesian_mixture_predict_predict_proba():\n+    # this is the same test as test_gaussian_mixture_predict_predict_proba()\n+    rng = np.random.RandomState(0)\n+    rand_data = RandomData(rng)\n+    for prior_type in PRIOR_TYPE:\n+        for covar_type in COVARIANCE_TYPE:\n+            X = rand_data.X[covar_type]\n+            Y = rand_data.Y\n+            bgmm = BayesianGaussianMixture(\n+                n_components=rand_data.n_components,\n+                random_state=rng,\n+                weight_concentration_prior_type=prior_type,\n+                covariance_type=covar_type)\n+\n+            # Check a warning message arrive if we don't do fit\n+            assert_raise_message(NotFittedError,\n+                                 \"This BayesianGaussianMixture instance\"\n+                                 \" is not fitted yet. Call 'fit' with \"\n+                                 \"appropriate arguments before using \"\n+                                 \"this method.\", bgmm.predict, X)\n+\n+            bgmm.fit(X)\n+            Y_pred = bgmm.predict(X)\n+            Y_pred_proba = bgmm.predict_proba(X).argmax(axis=1)\n+            assert_array_equal(Y_pred, Y_pred_proba)\n+            assert_greater_equal(adjusted_rand_score(Y, Y_pred), .95)\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -3,6 +3,7 @@\n # License: BSD 3 clause\n \n import sys\n+import copy\n import warnings\n \n import numpy as np\n@@ -569,6 +570,26 @@ def test_gaussian_mixture_predict_predict_proba():\n         assert_greater(adjusted_rand_score(Y, Y_pred), .95)\n \n \n+def test_gaussian_mixture_fit_predict():\n+    rng = np.random.RandomState(0)\n+    rand_data = RandomData(rng)\n+    for covar_type in COVARIANCE_TYPE:\n+        X = rand_data.X[covar_type]\n+        Y = rand_data.Y\n+        g = GaussianMixture(n_components=rand_data.n_components,\n+                            random_state=rng, weights_init=rand_data.weights,\n+                            means_init=rand_data.means,\n+                            precisions_init=rand_data.precisions[covar_type],\n+                            covariance_type=covar_type)\n+\n+        # check if fit_predict(X) is equivalent to fit(X).predict(X)\n+        f = copy.deepcopy(g)\n+        Y_pred1 = f.fit(X).predict(X)\n+        Y_pred2 = g.fit_predict(X)\n+        assert_array_equal(Y_pred1, Y_pred2)\n+        assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n+\n+\n def test_gaussian_mixture_fit():\n     # recover the ground truth\n     rng = np.random.RandomState(0)\n",
        "repo": "scikit-learn/scikit-learn",
        "problem_statement": "Should mixture models have a clusterer-compatible interface\nMixture models are currently a bit different. They are basically clusterers, except they are probabilistic, and are applied to inductive problems unlike many clusterers. But they are unlike clusterers in API:\r\n* they have an `n_components` parameter, with identical purpose to `n_clusters`\r\n* they do not store the `labels_` of the training data\r\n* they do not have a `fit_predict` method\r\n\r\nAnd they are almost entirely documented separately.\r\n\r\nShould we make the MMs more like clusterers?\n",
        "version": "0.20",
        "FAIL_TO_PASS": [
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_fit_predict",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_predict"
        ],
        "PASS_TO_PASS": [
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_check_is_fitted",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_covariance_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_means_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_precisions_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_predict_predict_proba",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weight_concentration_prior_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_bayesian_mixture_weights_prior_initialisation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_check_covariance_precision",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_compare_covar_type",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_invariant_translation",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_log_dirichlet_norm",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_log_wishart_norm",
            "sklearn/mixture/tests/test_bayesian_mixture.py::test_monotonic_likelihood",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_bic_1d_1component",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_X",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_means",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_precisions",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_check_weights",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_compute_log_det_cholesky",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_aic_bic",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_attributes",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_estimate_log_prob_resp",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_best_params",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_fit_convergence_warning",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_log_probabilities",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_n_parameters",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_predict_predict_proba",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_mixture_verbose",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_gaussian_suffstat_sk_spherical",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_init",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_monotonic_likelihood",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_multiple_init",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_property",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_regularisation",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_sample",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_score",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_score_samples",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_diag",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_full",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_suffstat_sk_tied",
            "sklearn/mixture/tests/test_gaussian_mixture.py::test_warm_start"
        ],
        "environment_setup_commit": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1",
        "patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -172,7 +172,7 @@ def _initialize(self, X, resp):\n     def fit(self, X, y=None):\n         \"\"\"Estimate model parameters with the EM algorithm.\n \n-        The method fit the model `n_init` times and set the parameters with\n+        The method fits the model `n_init` times and set the parameters with\n         which the model has the largest likelihood or lower bound. Within each\n         trial, the method iterates between E-step and M-step for `max_iter`\n         times until the change of likelihood or lower bound is less than\n@@ -188,6 +188,32 @@ def fit(self, X, y=None):\n         -------\n         self\n         \"\"\"\n+        self.fit_predict(X, y)\n+        return self\n+\n+    def fit_predict(self, X, y=None):\n+        \"\"\"Estimate model parameters using X and predict the labels for X.\n+\n+        The method fits the model n_init times and sets the parameters with\n+        which the model has the largest likelihood or lower bound. Within each\n+        trial, the method iterates between E-step and M-step for `max_iter`\n+        times until the change of likelihood or lower bound is less than\n+        `tol`, otherwise, a `ConvergenceWarning` is raised. After fitting, it\n+        predicts the most probable label for the input data points.\n+\n+        .. versionadded:: 0.20\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n         X = _check_X(X, self.n_components, ensure_min_samples=2)\n         self._check_initial_parameters(X)\n \n@@ -240,7 +266,7 @@ def fit(self, X, y=None):\n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n \n-        return self\n+        return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n",
        "pr_link": "https://github.com/scikit-learn/scikit-learn/pull/11281"
    }
}