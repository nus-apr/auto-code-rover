{
    "task_id": "pytest-dev__pytest-7373",
    "setup_info": {
        "repo_path": "/home/haifeng/projects/reverse-prompt/SWE-bench/testbed/pytest-dev__pytest/setup_pytest-dev__pytest__5.4",
        "env_name": "setup_pytest-dev__pytest__5.4",
        "pre_install": [],
        "install": "python -m pip install -e .",
        "test_cmd": "pytest -rA testing/test_mark.py"
    },
    "task_info": {
        "instance_id": "pytest-dev__pytest-7373",
        "base_commit": "7b77fc086aab8b3a8ebc890200371884555eea1e",
        "hints_text": "> I think the most appropriate fix is to simply remove the caching, which I don't think is necessary really, and inline cached_eval into MarkEvaluator._istrue.\r\n\r\nI agree:\r\n\r\n* While it might have some performance impact with very large test suites which use marks with eval, the simple workaround is to not use the eval feature on those, which is more predictable anyway.\r\n* I don't see a clean way to turn \"globals\" in some kind of cache key without having some performance impact and/or adverse effects.\r\n\r\nSo \ud83d\udc4d from me to simply removing this caching. \nAs globals are dynamic, i would propose to drop the cache as well, we should investigate reinstating a cache later on ",
        "created_at": "2020-06-15T17:12:08Z",
        "test_patch": "diff --git a/testing/test_mark.py b/testing/test_mark.py\n--- a/testing/test_mark.py\n+++ b/testing/test_mark.py\n@@ -706,6 +706,36 @@ def test_1(parameter):\n         reprec = testdir.inline_run()\n         reprec.assertoutcome(skipped=1)\n \n+    def test_reevaluate_dynamic_expr(self, testdir):\n+        \"\"\"#7360\"\"\"\n+        py_file1 = testdir.makepyfile(\n+            test_reevaluate_dynamic_expr1=\"\"\"\n+            import pytest\n+\n+            skip = True\n+\n+            @pytest.mark.skipif(\"skip\")\n+            def test_should_skip():\n+                assert True\n+        \"\"\"\n+        )\n+        py_file2 = testdir.makepyfile(\n+            test_reevaluate_dynamic_expr2=\"\"\"\n+            import pytest\n+\n+            skip = False\n+\n+            @pytest.mark.skipif(\"skip\")\n+            def test_should_not_skip():\n+                assert True\n+        \"\"\"\n+        )\n+\n+        file_name1 = os.path.basename(py_file1.strpath)\n+        file_name2 = os.path.basename(py_file2.strpath)\n+        reprec = testdir.inline_run(file_name1, file_name2)\n+        reprec.assertoutcome(passed=1, skipped=1)\n+\n \n class TestKeywordSelection:\n     def test_select_simple(self, testdir):\n",
        "repo": "pytest-dev/pytest",
        "problem_statement": "Incorrect caching of skipif/xfail string condition evaluation\nVersion: pytest 5.4.3, current master\r\n\r\npytest caches the evaluation of the string in e.g. `@pytest.mark.skipif(\"sys.platform == 'win32'\")`. The caching key is only the string itself (see `cached_eval` in `_pytest/mark/evaluate.py`). However, the evaluation also depends on the item's globals, so the caching can lead to incorrect results. Example:\r\n\r\n```py\r\n# test_module_1.py\r\nimport pytest\r\n\r\nskip = True\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_skip():\r\n    assert False\r\n```\r\n\r\n```py\r\n# test_module_2.py\r\nimport pytest\r\n\r\nskip = False\r\n\r\n@pytest.mark.skipif(\"skip\")\r\ndef test_should_not_skip():\r\n    assert False\r\n```\r\n\r\nRunning `pytest test_module_1.py test_module_2.py`.\r\n\r\nExpected: `test_should_skip` is skipped, `test_should_not_skip` is not skipped.\r\n\r\nActual: both are skipped.\r\n\r\n---\r\n\r\nI think the most appropriate fix is to simply remove the caching, which I don't think is necessary really, and inline `cached_eval` into `MarkEvaluator._istrue`.\n",
        "version": "5.4",
        "FAIL_TO_PASS": [
            "testing/test_mark.py::TestFunctional::test_reevaluate_dynamic_expr"
        ],
        "PASS_TO_PASS": [
            "testing/test_mark.py::TestFunctional::test_keyword_added_for_session",
            "testing/test_mark.py::TestFunctional::test_keywords_at_node_level",
            "testing/test_mark.py::TestFunctional::test_mark_closest",
            "testing/test_mark.py::TestFunctional::test_mark_decorator_baseclasses_merged",
            "testing/test_mark.py::TestFunctional::test_mark_decorator_subclass_does_not_propagate_to_base",
            "testing/test_mark.py::TestFunctional::test_mark_dynamically_in_funcarg",
            "testing/test_mark.py::TestFunctional::test_mark_from_parameters",
            "testing/test_mark.py::TestFunctional::test_mark_should_not_pass_to_siebling_class",
            "testing/test_mark.py::TestFunctional::test_mark_with_wrong_marker",
            "testing/test_mark.py::TestFunctional::test_merging_markers_deep",
            "testing/test_mark.py::TestFunctional::test_no_marker_match_on_unmarked_names",
            "testing/test_mark.py::TestKeywordSelection::test_keyword_extra",
            "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[+]",
            "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[..]",
            "testing/test_mark.py::TestKeywordSelection::test_no_magic_values[__]",
            "testing/test_mark.py::TestKeywordSelection::test_no_match_directories_outside_the_suite",
            "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass",
            "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[TestClass]",
            "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx",
            "testing/test_mark.py::TestKeywordSelection::test_select_extra_keywords[xxx]",
            "testing/test_mark.py::TestKeywordSelection::test_select_simple",
            "testing/test_mark.py::TestKeywordSelection::test_select_starton",
            "testing/test_mark.py::TestMark::test_mark_with_param",
            "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-mark]",
            "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[py.test-param]",
            "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-mark]",
            "testing/test_mark.py::TestMark::test_pytest_exists_in_namespace_all[pytest-param]",
            "testing/test_mark.py::TestMark::test_pytest_mark_name_starts_with_underscore",
            "testing/test_mark.py::TestMark::test_pytest_mark_notcallable",
            "testing/test_mark.py::TestMarkDecorator::test__eq__[foo-rhs3-False]",
            "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs0-rhs0-True]",
            "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs1-rhs1-False]",
            "testing/test_mark.py::TestMarkDecorator::test__eq__[lhs2-bar-False]",
            "testing/test_mark.py::TestMarkDecorator::test_aliases",
            "testing/test_mark.py::test_addmarker_order",
            "testing/test_mark.py::test_ini_markers",
            "testing/test_mark.py::test_ini_markers_whitespace",
            "testing/test_mark.py::test_keyword_option_considers_mark",
            "testing/test_mark.py::test_keyword_option_custom[1",
            "testing/test_mark.py::test_keyword_option_custom[interface-expected_passed0]",
            "testing/test_mark.py::test_keyword_option_custom[not",
            "testing/test_mark.py::test_keyword_option_custom[pass-expected_passed2]",
            "testing/test_mark.py::test_keyword_option_parametrize[2-3-expected_passed2]",
            "testing/test_mark.py::test_keyword_option_parametrize[None-expected_passed0]",
            "testing/test_mark.py::test_keyword_option_parametrize[[1.3]-expected_passed1]",
            "testing/test_mark.py::test_keyword_option_wrong_arguments[(foo-at",
            "testing/test_mark.py::test_keyword_option_wrong_arguments[foo",
            "testing/test_mark.py::test_keyword_option_wrong_arguments[not",
            "testing/test_mark.py::test_keyword_option_wrong_arguments[or",
            "testing/test_mark.py::test_mark_expressions_no_smear",
            "testing/test_mark.py::test_mark_on_pseudo_function",
            "testing/test_mark.py::test_mark_option[(((",
            "testing/test_mark.py::test_mark_option[not",
            "testing/test_mark.py::test_mark_option[xyz",
            "testing/test_mark.py::test_mark_option[xyz-expected_passed0]",
            "testing/test_mark.py::test_mark_option[xyz2-expected_passed4]",
            "testing/test_mark.py::test_mark_option_custom[interface-expected_passed0]",
            "testing/test_mark.py::test_mark_option_custom[not",
            "testing/test_mark.py::test_marked_class_run_twice",
            "testing/test_mark.py::test_marker_expr_eval_failure_handling[NOT",
            "testing/test_mark.py::test_marker_expr_eval_failure_handling[bogus/]",
            "testing/test_mark.py::test_marker_without_description",
            "testing/test_mark.py::test_markers_from_parametrize",
            "testing/test_mark.py::test_markers_option",
            "testing/test_mark.py::test_markers_option_with_plugin_in_current_dir",
            "testing/test_mark.py::test_parameterset_for_fail_at_collect",
            "testing/test_mark.py::test_parameterset_for_parametrize_bad_markname",
            "testing/test_mark.py::test_parameterset_for_parametrize_marks[None]",
            "testing/test_mark.py::test_parameterset_for_parametrize_marks[]",
            "testing/test_mark.py::test_parameterset_for_parametrize_marks[skip]",
            "testing/test_mark.py::test_parameterset_for_parametrize_marks[xfail]",
            "testing/test_mark.py::test_parametrize_iterator",
            "testing/test_mark.py::test_parametrize_with_module",
            "testing/test_mark.py::test_parametrized_collect_with_wrong_args",
            "testing/test_mark.py::test_parametrized_collected_from_command_line",
            "testing/test_mark.py::test_parametrized_with_kwargs",
            "testing/test_mark.py::test_pytest_param_id_allows_none_or_string[None]",
            "testing/test_mark.py::test_pytest_param_id_allows_none_or_string[hello",
            "testing/test_mark.py::test_pytest_param_id_requires_string",
            "testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict-markers]",
            "testing/test_mark.py::test_strict_prohibits_unregistered_markers[--strict]"
        ],
        "environment_setup_commit": "678c1a0745f1cf175c442c719906a1f13e496910",
        "patch": "diff --git a/src/_pytest/mark/evaluate.py b/src/_pytest/mark/evaluate.py\n--- a/src/_pytest/mark/evaluate.py\n+++ b/src/_pytest/mark/evaluate.py\n@@ -10,25 +10,14 @@\n from ..outcomes import fail\n from ..outcomes import TEST_OUTCOME\n from .structures import Mark\n-from _pytest.config import Config\n from _pytest.nodes import Item\n-from _pytest.store import StoreKey\n \n \n-evalcache_key = StoreKey[Dict[str, Any]]()\n+def compiled_eval(expr: str, d: Dict[str, object]) -> Any:\n+    import _pytest._code\n \n-\n-def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n-    default = {}  # type: Dict[str, object]\n-    evalcache = config._store.setdefault(evalcache_key, default)\n-    try:\n-        return evalcache[expr]\n-    except KeyError:\n-        import _pytest._code\n-\n-        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n-        evalcache[expr] = x = eval(exprcode, d)\n-        return x\n+    exprcode = _pytest._code.compile(expr, mode=\"eval\")\n+    return eval(exprcode, d)\n \n \n class MarkEvaluator:\n@@ -98,7 +87,7 @@ def _istrue(self) -> bool:\n                     self.expr = expr\n                     if isinstance(expr, str):\n                         d = self._getglobals()\n-                        result = cached_eval(self.item.config, expr, d)\n+                        result = compiled_eval(expr, d)\n                     else:\n                         if \"reason\" not in mark.kwargs:\n                             # XXX better be checked at collection time\n",
        "pr_link": "https://github.com/pytest-dev/pytest/pull/7373"
    }
}